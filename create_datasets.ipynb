{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zeros(positions, transfer_integrals):\n",
    "    '''removes zeros from transfer integrals, so that we can take the log during processing'''\n",
    "    mask = ~(transfer_integrals==0.)\n",
    "    transfer_integrals = transfer_integrals[mask]\n",
    "    positions = positions[mask]\n",
    "    return positions, transfer_integrals\n",
    "\n",
    "def split(positions, transfer_integrals, len_train=0, len_test=0):\n",
    "    '''shuffle and split the dataset into a train and a test part'''\n",
    "    permutation = np.random.permutation(len(transfer_integrals))\n",
    "    positions = positions[permutation]\n",
    "    transfer_integrals = transfer_integrals[permutation]\n",
    "    positions_train = positions[:len_train]\n",
    "    positions_test = positions[len_train:(len_train+len_test)]\n",
    "    transfer_integrals_train = transfer_integrals[:len_train]\n",
    "    transfer_integrals_test = transfer_integrals[len_train:(len_train+len_test)]\n",
    "    return positions_train, positions_test, transfer_integrals_train, transfer_integrals_test\n",
    "\n",
    "\n",
    "'''shuffles and splits dataset into train and test (so that they do not share )'''\n",
    "def read_file(name):\n",
    "    '''reads original .h5 file'''\n",
    "    with h5py.File('data/data_original/raw/john_'+name+'.h5', 'r') as file:\n",
    "        atoms = file.get(\"atoms\")[()]\n",
    "        positions = file.get(\"positions\")[()]\n",
    "        transfer_integrals = file.get(\"transfer_integrals\")[()]\n",
    "        return atoms, positions, transfer_integrals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug File penta 100, tetra 100, DNTT 50 (train), DNTT 50 (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset: (for each molecule) read file, remove zeros and split, append to file\n",
    "# pentacene\n",
    "name = 'pentacene' \n",
    "len_train = 100\n",
    "len_test = 0\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'debug_penta_100_tetra_100_DNTT_50_train.h5'\n",
    "path = os.path.join('data','raw')\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'w') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tetra \n",
    "name = 'tetracene' \n",
    "len_train = 100\n",
    "len_test = 0\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'debug_penta_100_tetra_100_DNTT_50_train.h5'\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'a') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNTT \n",
    "name = 'DNTT' \n",
    "len_train = 50\n",
    "len_test = 50\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'debug_penta_100_tetra_100_DNTT_50_train.h5'\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'a') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'debug_DNTT_50_test.h5'\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'w') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug data: penta 100 (train), tetra 50 (test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentacene\n",
    "name = 'pentacene' \n",
    "len_train = 100\n",
    "len_test = 0\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'debug_penta_100_train.h5'\n",
    "path = os.path.join('data','raw')\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'w') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tetra \n",
    "name = 'tetracene' \n",
    "len_train = 0\n",
    "len_test = 50\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'debug_tetra_50_test.h5'\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'w') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug data: penta 50 (test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentacene\n",
    "name = 'pentacene' \n",
    "len_train = 0 \n",
    "len_test = 50\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'debug_penta_50_test.h5'\n",
    "path = os.path.join('data','raw')\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'w') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data for Exp 7 (train)\n",
    "#### penta 15000, tetra 5000 (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentacene\n",
    "name = 'pentacene' \n",
    "len_train = 15000\n",
    "len_test = 0\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'penta_15k_tetra_5k_train.h5'\n",
    "path = os.path.join('data','raw')\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'w') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tetra \n",
    "name = 'tetracene' \n",
    "len_train = 5000\n",
    "len_test = 0\n",
    "\n",
    "# read \n",
    "atoms, positions, transfer_integrals = read_file(name)\n",
    "(positions, transfer_integrals) = remove_zeros(positions, transfer_integrals)\n",
    "(positions_train, \n",
    " positions_test, \n",
    " transfer_integrals_train, \n",
    " transfer_integrals_test) = split(positions, transfer_integrals, len_train=len_train, len_test=len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "write_name = 'penta_15k_tetra_5k_train.h5'\n",
    "path = os.path.join('data','raw')\n",
    "\n",
    "if \"train\" in write_name:\n",
    "    positions = positions_train\n",
    "    transfer_integrals = transfer_integrals_train\n",
    "elif \"test\" in write_name:\n",
    "    positions = positions_test\n",
    "    transfer_integrals = transfer_integrals_test\n",
    "\n",
    "with h5py.File(os.path.join(path, write_name), 'a') as file:\n",
    "    grp = file.create_group(name)\n",
    "    grp.create_dataset('atoms', data=atoms)\n",
    "    grp.create_dataset('positions', data=positions)\n",
    "    grp.create_dataset('transfer_integrals', data=transfer_integrals)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pentacene\n",
      "length: 15000\n",
      "tetracene\n",
      "length: 5000\n"
     ]
    }
   ],
   "source": [
    "# read \n",
    "write_name = 'penta_15k_tetra_5k_train.h5'\n",
    "path = os.path.join('data','raw')\n",
    "\n",
    "atoms= {} \n",
    "positions= {}\n",
    "transfer_integrals= {}\n",
    "with h5py.File(os.path.join(path, write_name), \"r\") as file: \n",
    "    groups = list(file.keys())\n",
    "    for group in groups:\n",
    "        # list top-level groups/datasets in file\n",
    "\n",
    "        # get content and convert to numpy arrays\n",
    "        atoms[group] = file[group].get(\"atoms\")[()].astype(str)\n",
    "        positions[group] = file[group].get(\"positions\")[()]\n",
    "        transfer_integrals[group] = file[group].get(\"transfer_integrals\")[()] \n",
    "        print(group)\n",
    "        print(f'length: {len(transfer_integrals[group])}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domain_adaptation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
